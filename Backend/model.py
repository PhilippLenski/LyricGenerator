from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# üìå Modell und Tokenizer laden
MODEL_PATH = "/app/models/google/gemma-3-4b-it"

# **Modell und Tokenizer von Hugging Face laden**
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map="auto", torch_dtype=torch.float16)

def generate_poem(city: str) -> str:
    """Generiert ein humorvolles Gedicht √ºber eine Stadt mit Gemma 3 4B IT."""
    prompt = f"""
    Schreibe ein humorvolles Gedicht √ºber {city}, das sich auf die lokalen Ess- und Trinkgewohnheiten bezieht.
    Das Gedicht muss genau eine Strophe mit vier Zeilen enthalten.
    Verwende das Kreuzreim-Schema (abab).
    Die S√§tze d√ºrfen nicht auf das gleiche Wort enden und m√ºssen sich sinnvoll reimen.

    Beispiel:
    **Hamburg**
    Frischer Fisch vom Hafensteg,
    ein Astra k√ºhlt die Seele fein.
    Labskaus liegt mir oft im Magen,
    doch 'nen Franzbr√∂tchen darf es sein.

    Jetzt schreibe ein Gedicht √ºber {city}:
    """

    # **Tokenisierung**
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")

    # **Generierung**
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_length=100,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            repetition_penalty=1.2
        )

    # **Ergebnis dekodieren**
    poem = tokenizer.decode(output[0], skip_special_tokens=True)

    # **Nur den relevanten Teil zur√ºckgeben**
    return poem.split("Jetzt schreibe ein Gedicht √ºber")[-1].strip()